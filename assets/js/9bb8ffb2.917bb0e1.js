"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7853],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),h=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=h(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=h(n),m=r,f=u["".concat(l,".").concat(m)]||u[m]||p[m]||i;return n?a.createElement(f,o(o({ref:t},c),{},{components:n})):a.createElement(f,o({ref:t},c))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,o[1]=s;for(var h=2;h<i;h++)o[h]=n[h];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4620:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>h});var a=n(7462),r=(n(7294),n(3905));const i={sidebar_position:3},o="OpenAI Function Calling",s={unversionedId:"llms/open_ai_functions",id:"llms/open_ai_functions",title:"OpenAI Function Calling",description:"By default, LLMs take text as input, and product text as output, but when we are building LLM applications, many times we want some specific outputs from the LLM, or to fork the execution flow to take the user in another direction. One way to do that, is to ask the LLM to produce a JSON, and the try to parse that JSON. Problem is, often times this JSON can be invalid, and it's a bit hassle to work with it.",source:"@site/docs/llms/open_ai_functions.md",sourceDirName:"llms",slug:"/llms/open_ai_functions",permalink:"/litechain/docs/llms/open_ai_functions",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llms/open_ai_functions.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"OpenAI LLMs",permalink:"/litechain/docs/llms/open_ai"},next:{title:"GPT4All LLMs",permalink:"/litechain/docs/llms/gpt4all"}},l={},h=[],c={toc:h},u="wrapper";function p(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"openai-function-calling"},"OpenAI Function Calling"),(0,r.kt)("p",null,"By default, LLMs take text as input, and product text as output, but when we are building LLM applications, many times we want some specific outputs from the LLM, or to fork the execution flow to take the user in another direction. One way to do that, is to ask the LLM to produce a JSON, and the try to parse that JSON. Problem is, often times this JSON can be invalid, and it's a bit hassle to work with it."),(0,r.kt)("p",null,"So OpenAI developed a feature that constrains the logits to produce a valid structure",(0,r.kt)("a",{parentName:"p",href:"https://github.com/newhouseb/clownfish/"},"[1]"),", effectively getting them to create a valid schema for doing function calling, which enables us to get structured output and routing effortlessly. You can read more about it in their ",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/blog/function-calling-and-other-api-updates"},"official announcement"),"."),(0,r.kt)("p",null,"LiteChain then integrates OpenAI functions even more deeply, allowing you to pass actual Python functions and call other chains from it."),(0,r.kt)("p",null,"To pass a function for ",(0,r.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAIChatChain"},(0,r.kt)("inlineCode",{parentName:"a"},"OpenAIChatChain"))," to call, first you will have to declare it with parameter ",(0,r.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/typing.html"},"types")," and ",(0,r.kt)("a",{parentName:"p",href:"https://www.programiz.com/python-programming/docstrings"},"docstrings"),", those are essential for the model to know when should it call this function, what is the type it should produce for each parameter, and what each parameter is about. Writing it well is like writing your prompts well, because that's what the model will be reading, if something is missing, you will get a runtime error."),(0,r.kt)("p",null,"Declare your function like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import TypedDict, Literal\n\nclass WeatherReturn(TypedDict):\n    location: str\n    forecast: str\n    temperature: str\n\ndef get_current_weather(\n    location: str, format: Literal["celsius", "fahrenheit"] = "celsius"\n) -> WeatherReturn:\n    """\n    Gets the current weather in a given location, use this function for any questions related to the weather\n\n    Parameters\n    ----------\n    location\n        The city to get the weather, e.g. San Francisco. Guess the location from user messages\n\n    format\n        A string with the full content of what the given role said\n    """\n\n    return WeatherReturn(\n        location=location,\n        forecast="sunny",\n        temperature="25 C" if format == "celsius" else "77 F",\n    )\n')),(0,r.kt)("p",null,"Notice how we are using ",(0,r.kt)("inlineCode",{parentName:"p"},'Literal["celsius", "fahrenheit"]')," in the ",(0,r.kt)("inlineCode",{parentName:"p"},"format")," parameter, this works as an enum, and limits the values that the model can inject there. Notice also the full docstring, documenting what the function does and why it should be called, and each of the parameters. You don't need to repeat the parameter types on the docstring because they are already defined on the function signature."),(0,r.kt)("p",null,"Finally, we create the ",(0,r.kt)("inlineCode",{parentName:"p"},"WeatherReturn")," for ourselves to organize the output, but the return type will not really be considered by the model."),(0,r.kt)("p",null,"Now that you have a proper documented function, you can simply pass it to ",(0,r.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAIChatChain"},(0,r.kt)("inlineCode",{parentName:"a"},"OpenAIChatChain")),", and it will be smart enough to know when to use it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from typing import Union\nfrom litechain import collect_final_output\nfrom litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta\n\nchain = OpenAIChatChain[str, Union[OpenAIChatDelta, WeatherReturn]](\n    \"WeatherChain\",\n    lambda user_input: [\n        OpenAIChatMessage(role=\"user\", content=user_input),\n    ],\n    model=\"gpt-3.5-turbo\",\n    functions=[get_current_weather],\n    temperature=0,\n)\n\nawait collect_final_output(\n    chain(\n        \"I'm in my appartment in Amsterdam, thinking... should I take an umbrella for my pet chicken?\"\n    )\n)\n# [{'location': 'Amsterdam', 'forecast': 'sunny', 'temperature': '25 C'}]\n")),(0,r.kt)("p",null,"Notice how the output type of the chain is ",(0,r.kt)("inlineCode",{parentName:"p"},"Union[OpenAIChatDelta, WeatherReturn]"),', this is because the chain now can return either a simple message reply, if the user says "hello, what\'s up" for example, or it may return a ',(0,r.kt)("inlineCode",{parentName:"p"},"WeatherReturn")," because they user has asked about the weather and therefore called the function. The return types get wired up automatically by the type system so you get no surprises of what things you are possibly getting back from the chain."),(0,r.kt)("p",null,"Now, to go one step beyond, not necessarily you function has to return simple values, you can also call other chains from it, so effectively one LLM is calling the other, directly! \ud83e\udd2f Check out this example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import TypedDict, Literal, Union\nfrom litechain import debug, collect_final_output\nfrom litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta\nimport json\n\nclass WeatherReturn(TypedDict):\n    location: str\n    forecast: str\n    temperature: str\n\ndef chain(user_input: str):\n    def reply_with_current_weather(\n        location: str, format: Literal["celsius", "fahrenheit"] = "celsius"\n    ):\n        """\n        Gets the current weather in a given location and replies user, use this function for any questions related to the weather"\n\n        Parameters\n        ----------\n        location\n            The city to get the weather, e.g. San Francisco. Guess the location from user messages\n\n        format\n            A string with the full content of what the given role said\n        """\n\n        weather: WeatherReturn = {\n            "location": location,\n            "forecast": "sunny",\n            "temperature": "25 C" if format == "celsius" else "77 F",\n        }\n\n        return weather_reply_chain(weather)\n\n    weather_chain = OpenAIChatChain[str, OpenAIChatDelta](\n        "WeatherChain",\n        lambda user_input: [\n            OpenAIChatMessage(role="user", content=user_input),\n        ],\n        model="gpt-3.5-turbo",\n        functions=[reply_with_current_weather],\n        temperature=0,\n    )\n\n    weather_reply_chain = OpenAIChatChain[WeatherReturn, OpenAIChatDelta](\n        "WeatherReplyChain",\n        lambda weather: [\n            OpenAIChatMessage(role="user", content=user_input),\n            OpenAIChatMessage(\n                role="user",\n                content=f"Output from the weather system: {json.dumps(weather)}",\n            ),\n        ],\n        model="gpt-3.5-turbo",\n        temperature=0,\n    )\n\n    return debug(weather_chain)(user_input)\n\nawait collect_final_output(\n    chain(\n        "I\'m in my appartment in Amsterdam, thinking... should I take an umbrella for my pet chicken?"\n    )\n)\n# > WeatherChain\n#\n# Function: reply_with_current_weather(location=\'Amsterdam\')\n#\n# > WeatherReplyChain\n#\n# Assistant: Based on the current weather forecast in Amsterdam, it is sunny with a temperature of 25\xb0C. Since it is not raining, you do not need to take an umbrella for your pet chicken. Enjoy the sunny weather!\n')),(0,r.kt)("p",null,"Here we used the ",(0,r.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/utils/chain.html#litechain.utils.chain.debug"},(0,r.kt)("inlineCode",{parentName:"a"},"debug"))," function to show what is going on, you can see it first called the function, and then it gave a proper reply about the Amsterdam weather."),(0,r.kt)("p",null,"The way it works is that we create a function ",(0,r.kt)("inlineCode",{parentName:"p"},"chain"),", which takes the ",(0,r.kt)("inlineCode",{parentName:"p"},"user_input"),", creates the ",(0,r.kt)("inlineCode",{parentName:"p"},"reply_with_current_weather")," function and two chains, the ",(0,r.kt)("inlineCode",{parentName:"p"},"weather_chain")," and the ",(0,r.kt)("inlineCode",{parentName:"p"},"weather_reply_chain"),". The first chain just take the use input and find the appropriate function to call, the function ",(0,r.kt)("inlineCode",{parentName:"p"},"reply_with_current_weather")," then actually fetches the weather and call the second chain ",(0,r.kt)("inlineCode",{parentName:"p"},"weather_reply_chain"),", which reuses the ",(0,r.kt)("inlineCode",{parentName:"p"},"user_input")," plus the weather output and generate the final reply."),(0,r.kt)("p",null,"That's it for OpenAI Function Calling, now if want to run an LLM locally, check it out the next guide on GPT4All."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/newhouseb/clownfish/"},"[1]"),": Long read if you are interested on how logits constraining work: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/newhouseb/clownfish/"},"https://github.com/newhouseb/clownfish/")))}p.isMDXComponent=!0}}]);