"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[216],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},h="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=p(n),m=a,u=h["".concat(s,".").concat(m)]||h[m]||d[m]||i;return n?o.createElement(u,r(r({ref:t},c),{},{components:n})):o.createElement(u,r({ref:t},c))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[h]="string"==typeof e?e:a,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3730:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var o=n(7462),a=(n(7294),n(3905));const i={sidebar_position:2},r="OpenAI LLMs",l={unversionedId:"llms/open_ai",id:"llms/open_ai",title:"OpenAI LLMs",description:"OpenAI took the world by storm with the launch of ChatGPT and GPT-4, at the point of this writing, they are still the smartest LLMs out there. To use them, first you will need to get an API key from OpenAI, and export it with:",source:"@site/docs/llms/open_ai.md",sourceDirName:"llms",slug:"/llms/open_ai",permalink:"/litechain/docs/llms/open_ai",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/llms/open_ai.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"LLMs",permalink:"/litechain/docs/llms/"},next:{title:"GPT4All LLMs",permalink:"/litechain/docs/llms/gpt4all"}},s={},p=[{value:"Text Completion",id:"text-completion",level:2},{value:"Chat Completion",id:"chat-completion",level:2}],c={toc:p},h="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(h,(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"openai-llms"},"OpenAI LLMs"),(0,a.kt)("p",null,"OpenAI took the world by storm with the launch of ChatGPT and GPT-4, at the point of this writing, they are still the smartest LLMs out there. To use them, first you will need to get an ",(0,a.kt)("a",{parentName:"p",href:"https://platform.openai.com"},"API key from OpenAI"),", and export it with:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"export OPENAI_API_KEY=<your key here>\n")),(0,a.kt)("p",null,"Then, LiteChain provides two thin wrapper layers for their APIs:"),(0,a.kt)("h2",{id:"text-completion"},"Text Completion"),(0,a.kt)("p",null,"OpenAI has two modes of generating text with LLMs, the first one, simple and more, is text completion, using GPT-3 derived models:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from litechain import join_final_output\nfrom litechain.contrib import OpenAICompletionChain\n\nrecipe_chain = OpenAICompletionChain[str, str](\n    "RecipeChain",\n    lambda recipe_name: f"Here is my {recipe_name} recipe: ",\n    model="davinci",\n)\n\nawait join_final_output(recipe_chain("instant noodles"))\n#=> \'\\xa01. Boil water 2. Add noodles 3. Add seasoning 4.\'\n')),(0,a.kt)("p",null,'This model is not specialized for chat as ChatGPT and GPT-4, but for completion, consider this while writing your prompts. In this case, we are taking the user input and making it part of a sentence for the model to complete ("Here is my..."), instead of asking a question for the model to answer.'),(0,a.kt)("p",null,"When you use ",(0,a.kt)("inlineCode",{parentName:"p"},"OpenAICompletionChain")," instead of a regular ",(0,a.kt)("inlineCode",{parentName:"p"},"Chain"),", the lambda function you pass should return the prompt you want for the model consume, and the stream of outputs will be generated by the LLM, given this prompt."),(0,a.kt)("p",null,"You also must specify which model to use, OpenAI has several variations of the GPT-3 text completion model, take a look at ",(0,a.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/models/gpt-3"},"their page")," to see which ones are available."),(0,a.kt)("p",null,"You also have other parameters you can pass to the chain like ",(0,a.kt)("inlineCode",{parentName:"p"},"temperature"),", which ",(0,a.kt)("a",{parentName:"p",href:"#"},"helps with development if you keep it at zero")," (TODO: add link), and ",(0,a.kt)("inlineCode",{parentName:"p"},"max_tokens"),", take a look at ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAICompletionChain"},"the reference")," to learn more."),(0,a.kt)("h2",{id:"chat-completion"},"Chat Completion"),(0,a.kt)("p",null,"The most popular and powerful OpenAI completion API, however, is the Chat Completion, which gives you access to ",(0,a.kt)("inlineCode",{parentName:"p"},"gpt-3.5-turbo")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"gpt-4")," models. It is a bit more work to work with, because it has defined roles, for ",(0,a.kt)("inlineCode",{parentName:"p"},"system"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"user"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"assistant")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"function")," (TODO: function yet to be supported). You define an ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAIChatChain"},(0,a.kt)("inlineCode",{parentName:"a"},"OpenAIChatChain"))," like this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from litechain import Chain, join_final_output\nfrom litechain.contrib import OpenAIChatChain, OpenAIChatMessage, OpenAIChatDelta\n\nrecipe_chain: Chain[str, str] = OpenAIChatChain[str, OpenAIChatDelta](\n    "RecipeChain",\n    lambda recipe_name: [\n        OpenAIChatMessage(\n            role="system",\n            content="You are ChefGPT, an assistant bot trained on all culinary knowledge of world\'s most proeminant Michelin Chefs",\n        ),\n        OpenAIChatMessage(\n            role="user",\n            content=f"Hello, could you write me a recipe for {recipe_name}?",\n        ),\n    ],\n    model="gpt-3.5-turbo",\n).map(lambda delta: delta.content)\n\nawait join_final_output(recipe_chain("instant noodles"))\n#=> "Of course! Here\'s a simple and delicious recipe for instant noodles:\\n\\nIngredients:\\n- 1 packet of instant noodles (your choice of flavor)\\n- 2 cups of water\\n- 1 tablespoon of vegetable oil\\n- 1 small onion, thinly sliced\\n- 1 clove of garlic, minced\\n- 1 small carrot, julienned\\n- 1/2 cup of sliced mushrooms\\n- 1/2 cup of shredded cabbage\\n- 2 tablespoons of soy sauce\\n- 1 teaspoon of sesame oil\\n- Optional toppings: sliced green onions, boiled egg, cooked chicken or shrimp, chili flakes\\n\\nInstructions:\\n1. In a medium-sized pot, bring the water to a boil. Add the instant noodles and cook according to the package instructions until they are al dente. Drain and set aside.\\n\\n2. In the same pot, heat the vegetable oil over medium heat. Add the sliced onion and minced garlic, and saut\xe9 until they become fragrant and slightly caramelized.\\n\\n3. Add the julienned carrot, sliced mushrooms, and shredded cabbage to the pot. Stir-fry for a few minutes until the vegetables are slightly softened.\\n\\n4. Add the cooked instant noodles to the pot and toss them with the vegetables.\\n\\n5. In a small bowl, mix together the soy sauce and sesame oil. Pour this mixture over the noodles and vegetables, and toss everything together until well combined.\\n\\n6. Cook for an additional 2-3 minutes, stirring occasionally, to allow the flavors to meld together.\\n\\n7. Remove the pot from heat and divide the noodles into serving bowls. Top with your desired toppings such as sliced green onions, boiled egg, cooked chicken or shrimp, and chili flakes.\\n\\n8. Serve the instant noodles hot and enjoy!\\n\\nFeel free to customize this recipe by adding your favorite vegetables or protein. Enjoy your homemade instant noodles!"\n')),(0,a.kt)("p",null,"This model is really optimized for answering questions and following guidance. If you look at the lambda function, we do not return a simple string for the prompt, but a list of ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAIChatMessage"},(0,a.kt)("inlineCode",{parentName:"a"},"OpenAIChatMessage")),"s, those hold the role and the content."),(0,a.kt)("p",null,"Then, if you look at the type signature of ",(0,a.kt)("inlineCode",{parentName:"p"},"OpenAIChatChain"),", you will notice that it takes a ",(0,a.kt)("inlineCode",{parentName:"p"},"str")," but it returns an ",(0,a.kt)("a",{parentName:"p",href:"pathname:///reference/litechain/contrib/index.html#litechain.contrib.OpenAIChatMessage"},(0,a.kt)("inlineCode",{parentName:"a"},"OpenAIChatDelta")),", this is what OpenAI's chat completion API streams back to us, it holds also the ",(0,a.kt)("inlineCode",{parentName:"p"},"role")," and the ",(0,a.kt)("inlineCode",{parentName:"p"},"content"),", so before joining the chain, we need to do a ",(0,a.kt)("inlineCode",{parentName:"p"},"map")," on the ",(0,a.kt)("inlineCode",{parentName:"p"},"delta.content")," to get strings back."),(0,a.kt)("p",null,"That's it for OpenAI LLMs, but if instead of an API you want to run an LLM locally, check it out the next guide on GPT4All"))}d.isMDXComponent=!0}}]);